{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bu4uIXzpRKbo"
      },
      "source": [
        "In this lab, we will be constructing and comparing various deep neural networks using Keras on the MNIST and MNIST Fashion datasets. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHS4_QdvR2FJ"
      },
      "source": [
        "First things first, we need to import the necessary libraries for our task. For this lab, we will be sticking to Keras (particularly the Tensorflow-Keras flavor) for as many tasks as possible. Since we're using Google Colab, the installation has already been taken care of."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HsyLLYSJRGec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.6.0\n",
            "Keras Version:  2.6.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "import tensorflow.keras as keras \n",
        "print(\"Keras Version: \", keras.__version__)\n",
        "import numpy as np # we'll need this for data preprocessing "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMB6p5YpS2VT"
      },
      "source": [
        "Next, lets import the standard MNIST dataset. MNIST contains grayscale images of handwritten digits (0-9). This dataset is a very popular benchmarking dataset and is frequently used as a \"sanity check\" to validate new machine learning techniques. \n",
        "\n",
        "MNIST is a multi-class classification dataset which means there are more than 2 classes (there are 10, exactly), and we can only classify one class as positive (as opposed to multi-label learning, which can have multiple positive classes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vv9UrMBfSmFi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_train.shape =  (60000, 28, 28, 1)\n",
            "y_train.shape =  (60000,)\n",
            "x_test.shape =  (10000, 28, 28, 1)\n",
            "y_test.shape =  (10000,)\n",
            "y_train.shape =  (60000,)\n",
            "y_test.shape =  (10000,)\n"
          ]
        }
      ],
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "#this dataset is already split into training and testing sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Make sure images have shape (28, 28, 1)\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "\n",
        "print(\"x_train.shape = \", x_train.shape)\n",
        "print(\"y_train.shape = \", y_train.shape)\n",
        "print(\"x_test.shape = \", x_test.shape)\n",
        "print(\"y_test.shape = \", y_test.shape)\n",
        "\n",
        "\n",
        "# The labels are currently a single array of values\n",
        "# We need to convert this into a one-hot encoded array\n",
        "# i.e., 5 -> [0,0,0,0,0,1,0,0,0,0]\n",
        "# We'll see why in a few cells\n",
        "\n",
        "y_train_categorical = keras.utils.to_categorical(y_train)\n",
        "y_test_categorical = keras.utils.to_categorical(y_test)\n",
        "print(\"y_train.shape = \", y_train.shape)\n",
        "print(\"y_test.shape = \", y_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoMVbohmUfPh"
      },
      "source": [
        "The training set contains 60,000 images that are 28x28 pixels-squared and the testing set contains 10,000 images that are 28x28 pixels-squared. \n",
        "\n",
        "With the data loaded, let's construct a \"simple\" feed-forward neural network and train it as a baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Sf68Co0iUJHZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "hidden (Dense)               (None, 100)               78500     \n",
            "_________________________________________________________________\n",
            "classification (Dense)       (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 79,510\n",
            "Trainable params: 79,510\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/15\n",
            "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2891 - accuracy: 0.9176 - val_loss: 0.1282 - val_accuracy: 0.9627\n",
            "Epoch 2/15\n",
            "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1323 - accuracy: 0.9614 - val_loss: 0.1039 - val_accuracy: 0.9693\n",
            "Epoch 3/15\n",
            "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0927 - accuracy: 0.9725 - val_loss: 0.0885 - val_accuracy: 0.9735\n",
            "Epoch 4/15\n",
            "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0704 - accuracy: 0.9793 - val_loss: 0.0895 - val_accuracy: 0.9737\n",
            "Epoch 5/15\n",
            "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0547 - accuracy: 0.9836 - val_loss: 0.0834 - val_accuracy: 0.9753\n",
            "Epoch 6/15\n",
            "1688/1688 [==============================] - 2s 998us/step - loss: 0.0430 - accuracy: 0.9874 - val_loss: 0.0823 - val_accuracy: 0.9750\n",
            "Epoch 7/15\n",
            "1688/1688 [==============================] - 2s 998us/step - loss: 0.0348 - accuracy: 0.9894 - val_loss: 0.0936 - val_accuracy: 0.9750\n",
            "Epoch 8/15\n",
            "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0286 - accuracy: 0.9912 - val_loss: 0.0819 - val_accuracy: 0.9757\n",
            "Epoch 9/15\n",
            "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0235 - accuracy: 0.9927 - val_loss: 0.0868 - val_accuracy: 0.9770\n",
            "Epoch 10/15\n",
            "1688/1688 [==============================] - 2s 988us/step - loss: 0.0190 - accuracy: 0.9944 - val_loss: 0.0846 - val_accuracy: 0.9775\n",
            "Epoch 11/15\n",
            "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0160 - accuracy: 0.9949 - val_loss: 0.0883 - val_accuracy: 0.9783\n",
            "Epoch 12/15\n",
            "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0135 - accuracy: 0.9958 - val_loss: 0.0809 - val_accuracy: 0.9812\n",
            "Epoch 13/15\n",
            "1688/1688 [==============================] - 2s 989us/step - loss: 0.0134 - accuracy: 0.9958 - val_loss: 0.0913 - val_accuracy: 0.9782\n",
            "Epoch 14/15\n",
            "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0097 - accuracy: 0.9969 - val_loss: 0.1010 - val_accuracy: 0.9785\n",
            "Epoch 15/15\n",
            "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0086 - accuracy: 0.9975 - val_loss: 0.1035 - val_accuracy: 0.9772\n",
            "0.9782\n"
          ]
        }
      ],
      "source": [
        "def create_ff_baseline():\n",
        "  # this layer flattens the input from (28x28) to an array of 784\n",
        "  # this is the only model that requires this step\n",
        "  ff_baseline = keras.Sequential(keras.layers.Flatten(input_shape=(28, 28)))\n",
        "  ff_baseline.add(keras.layers.Dense(100, activation=\"relu\", name=\"hidden\"))\n",
        "  ff_baseline.add(keras.layers.Dense(10, activation='softmax', name='classification'))\n",
        "  return ff_baseline\n",
        "\n",
        "ff_baseline = create_ff_baseline()\n",
        "# Keras requires models to be \"compiled\" before they can be viewed or trained\n",
        "ff_baseline.compile(optimizer=\"adam\", #standard optimizer\n",
        "                    loss=\"categorical_crossentropy\", #standard for multi-class problems\n",
        "                    metrics=[\"accuracy\"])\n",
        "# Print a textual summary of the model\n",
        "ff_baseline.summary()\n",
        "# Train the model - we will use the output of this function  later\n",
        "ff_baseline_history = ff_baseline.fit(x=x_train, \n",
        "                                      y=y_train_categorical, \n",
        "                                      epochs=15,\n",
        "                                      validation_split=0.1)\n",
        "\n",
        "# Evaluate the model.\n",
        "# We'll just evaluate accuracy for now. Later we'll abstract this to its own function\n",
        "# and evaluate accuracy, f1, and area under the ROC curve \n",
        "predictions_ff_baseline = ff_baseline.predict(x_test)\n",
        "# Convert the categorical predictions back to labels for calculating metrics\n",
        "predictions_ff_baseline = [np.argmax(pred) for pred in predictions_ff_baseline]\n",
        "m = tf.keras.metrics.Accuracy()\n",
        "m.update_state(y_test, predictions_ff_baseline)\n",
        "print(m.result().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXdiu0lqr-5B"
      },
      "source": [
        "Let's revisit this model, and add a callback. A callback is a function that performs some task at the end of each epoch. In our case, we are going to add a check to see if the model has overfit and end training if it has. This is called EarlyStopping.\n",
        "\n",
        "More callbacks can be found https://keras.io/api/callbacks/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AYbblgqDr-gV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_2 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "hidden (Dense)               (None, 100)               78500     \n",
            "_________________________________________________________________\n",
            "classification (Dense)       (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 79,510\n",
            "Trainable params: 79,510\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "1688/1688 [==============================] - 3s 1ms/step - loss: 0.2904 - accuracy: 0.9176 - val_loss: 0.1448 - val_accuracy: 0.9573\n",
            "Epoch 2/200\n",
            "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1369 - accuracy: 0.9597 - val_loss: 0.0991 - val_accuracy: 0.9728\n",
            "Epoch 3/200\n",
            "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0939 - accuracy: 0.9714 - val_loss: 0.0870 - val_accuracy: 0.9748\n",
            "Epoch 4/200\n",
            "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0704 - accuracy: 0.9792 - val_loss: 0.0893 - val_accuracy: 0.9747\n",
            "Epoch 5/200\n",
            "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0557 - accuracy: 0.9827 - val_loss: 0.0778 - val_accuracy: 0.9768\n",
            "Epoch 6/200\n",
            "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0445 - accuracy: 0.9863 - val_loss: 0.0840 - val_accuracy: 0.9770\n",
            "Epoch 7/200\n",
            "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0364 - accuracy: 0.9888 - val_loss: 0.0834 - val_accuracy: 0.9775\n",
            "Epoch 8/200\n",
            "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0296 - accuracy: 0.9908 - val_loss: 0.0831 - val_accuracy: 0.9775\n",
            "Epoch 9/200\n",
            "1688/1688 [==============================] - 2s 977us/step - loss: 0.0244 - accuracy: 0.9926 - val_loss: 0.0834 - val_accuracy: 0.9793\n",
            "Epoch 10/200\n",
            "1688/1688 [==============================] - 2s 978us/step - loss: 0.0211 - accuracy: 0.9933 - val_loss: 0.0899 - val_accuracy: 0.9777\n",
            "Epoch 00010: early stopping\n",
            "0.9757\n"
          ]
        }
      ],
      "source": [
        "ff_baseline_es = create_ff_baseline()\n",
        "# we are setting up the early stopping mechanism to monitor validation loss, and should\n",
        "# it reach an minima and attempt to start climbing (a sign of overfitting), stop the training\n",
        "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
        "\n",
        "# Keras requires models to be \"compiled\" before they can be viewed or trained\n",
        "ff_baseline_es.compile(optimizer=\"adam\", #standard optimizer\n",
        "                    loss=\"categorical_crossentropy\", #standard for multi-class problems\n",
        "                    metrics=[\"accuracy\"])\n",
        "# Print a textual summary of the model\n",
        "ff_baseline_es.summary()\n",
        "# Train the model - we will use the output of this function  later\n",
        "ff_baseline_es_history = ff_baseline_es.fit(x=x_train, \n",
        "                                      y=y_train_categorical, \n",
        "                                      epochs=200,\n",
        "                                      validation_split=0.1,\n",
        "                                      callbacks=[es]) #note the addition of the callback\n",
        "\n",
        "# Evaluate the model.\n",
        "# We'll just evaluate accuracy for now. Later we'll abstract this to its own function\n",
        "# and evaluate accuracy, f1, and area under the ROC curve \n",
        "predictions_ff_baseline = ff_baseline_es.predict(x_test)\n",
        "# Convert the categorical predictions back to labels for calculating metrics\n",
        "predictions_ff_baseline = [np.argmax(pred) for pred in predictions_ff_baseline]\n",
        "m = tf.keras.metrics.Accuracy()\n",
        "m.update_state(y_test, predictions_ff_baseline)\n",
        "print(m.result().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNZDZ2ZqyWtS"
      },
      "source": [
        "Now, let's create a simple convolutional neural net (CNN) that has 3 hidden layers. CNNs extract spatial features based on the values of surrounding pixels. This makes them powerful for image classification tasks compared to feed-forward/dense neural networks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkZaBDFJVKfQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_simple_cnn():\n",
        "  input_shape = (28,28,1)\n",
        "\n",
        "  simple_cnn = keras.Sequential()\n",
        "  simple_cnn.add(keras.layers.Conv2D(10, input_shape=input_shape, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\n",
        "  simple_cnn.add(keras.layers.Conv2D(50, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\n",
        "  simple_cnn.add(keras.layers.MaxPool2D(pool_size=(2,2)))\n",
        "  simple_cnn.add(keras.layers.Dropout(0.25))\n",
        "  simple_cnn.add(keras.layers.Flatten())\n",
        "  simple_cnn.add(keras.layers.Dense(500, activation='relu'))\n",
        "  simple_cnn.add(keras.layers.Dense(10, activation='softmax'))\n",
        "  return simple_cnn\n",
        "\n",
        "simple_cnn = create_simple_cnn()\n",
        "simple_cnn.compile(optimizer=\"adam\", #standard optimizer\n",
        "                    loss=\"categorical_crossentropy\", #standard for multi-class problems\n",
        "                    metrics=[\"accuracy\"])\n",
        "simple_cnn.summary()\n",
        "\n",
        "simple_cnn_history = simple_cnn.fit(x=x_train, \n",
        "                                    y=y_train_categorical, \n",
        "                                    epochs=5,\n",
        "                                    validation_split=0.1)\n",
        "\n",
        "# Evaluate the model.\n",
        "# We'll just evaluate accuracy for now. Later we'll abstract this to its own function\n",
        "# and evaluate accuracy, f1, and area under the ROC curve \n",
        "predictions_simple_cnn = simple_cnn.predict(x_test)\n",
        "# Convert the categorical predictions back to labels for calculating metrics\n",
        "predictions_simple_cnn = [np.argmax(pred) for pred in predictions_simple_cnn]\n",
        "m = tf.keras.metrics.Accuracy()\n",
        "m.update_state(y_test, predictions_simple_cnn)\n",
        "print(m.result().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjteIRdnnBgs"
      },
      "source": [
        "Next, we will examine the output of the fit functions that we saved earlier. The fit function returns a dictionary of training and validation accuracy that we can plot for evaluation purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtGGBN8SUxMp"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "print(ff_baseline_history.history.keys())\n",
        "# print(simple_cnn_history)\n",
        "\n",
        "def visualize_training(history_dict):\n",
        "  plt.plot(history_dict['accuracy'])\n",
        "  plt.plot(history_dict['val_accuracy'])\n",
        "  plt.title('Model Accuracy')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Training', 'Validation'], loc='upper left')\n",
        "  plt.show()\n",
        "  # summarize history for loss\n",
        "  plt.plot(history_dict['loss'])\n",
        "  plt.plot(history_dict['val_loss'])\n",
        "  plt.title('Model Loss')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Training', 'Validation'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "print(\"Feed-Forward Baseline: \")\n",
        "visualize_training(ff_baseline_history.history)\n",
        "print(\"Feed-Forward Baseline with EarlyStopping: \")\n",
        "visualize_training(ff_baseline_es_history.history)\n",
        "print(\"Simple CNN:\")\n",
        "visualize_training(simple_cnn_history.history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GU30zFYnoZdr"
      },
      "source": [
        "Now, let's define a function that calculates accuracy, F1 score, and area under the ROC curve for the predictions given true labels. We'll use Scikit-learns functions for calculating this for convienence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-s80tsJLoyRs"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "  # y_true: array of labels that are not one-hot encoded\n",
        "  # y_pred: array of neural network outputs. One 10-element array of probabilities per image \n",
        "  y_pred_labels = [np.argmax(pred) for pred in y_pred]\n",
        "  accuracy = accuracy_score(y_true=y_true, y_pred = y_pred_labels)\n",
        "  f1 = f1_score(y_true=y_true, y_pred = y_pred_labels, average='macro')\n",
        "  encoded = tf.keras.utils.to_categorical(y_true)\n",
        "  roc_auc = roc_auc_score(y_true=encoded, y_score = y_pred, multi_class='ovr')\n",
        "\n",
        "  return {\"Accuracy\": accuracy, \"F1\": f1, \"AUC\": roc_auc}\n",
        "\n",
        "predictions_ff_baseline = ff_baseline.predict(x_test)\n",
        "predictions_ff_baseline_es = ff_baseline_es.predict(x_test)\n",
        "predictions_simple_cnn = simple_cnn.predict(x_test)\n",
        "print(\"Feed-Forward Baseline: \", calculate_metrics(y_true=y_test, y_pred=predictions_ff_baseline))\n",
        "print(\"Feed-Forward Baseline w/ Early Stopping: \", calculate_metrics(y_true=y_test, y_pred=predictions_ff_baseline_es))\n",
        "print(\"Simple CNN: \", calculate_metrics(y_true=y_test, y_pred=predictions_simple_cnn) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtakwSnVtCBs"
      },
      "source": [
        "# Assignment\n",
        "Your turn: With the information available in this lab, create your own convolutional neural network that has better performance than the simple CNN demonstrated earlier. \n",
        "\n",
        "Resources:\n",
        "- Layers API https://keras.io/api/layers/\n",
        "- Compile Documentation https://keras.io/api/models/model_training_apis/ \n",
        "- Optimization Functions https://keras.io/api/optimizers/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TS20VyCNrHCA"
      },
      "outputs": [],
      "source": [
        "def create_custom_CNN():\n",
        "  # For you to complete\n",
        "  pass\n",
        "\n",
        "# modify below with callbacks\n",
        "custom_CNN = create_custom_CNN()\n",
        "\n",
        "# select the appropriate parameters for the compile function\n",
        "custom_CNN.compile(...)\n",
        "custom_CNN.summary()\n",
        "\n",
        "custom_CNN_hist = custom_CNN.fit(x=x_train, \n",
        "                                    y=y_train_categorical, \n",
        "                                    epochs=,\n",
        "                                    validation_split=0.1)\n",
        "\n",
        "visualize_training(custom_CNN_hist.history)\n",
        "\n",
        "predictions_custom = predict(x_test)\n",
        "print(\"Custom CNN: \", calculate_metrics(y_true=y_test, y_pred=predictions_custom))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XVRmsgcHYfr"
      },
      "source": [
        "**Extra Information:** There are several pre-built neural network architectures available in Keras for image processing. Although it is very overkill for the MNIST data, let's walk through how to use the VGG16 architecture for this problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "psC1X5iuIVMQ",
        "outputId": "0a3e0048-8ed5-4c9d-f9e0-dac6d6ee0f78"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-64c55b16038a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# First, we need to resize the images to 32x32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvgg16_xtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvgg16_xtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvgg16_xtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvgg16_xtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-64c55b16038a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# First, we need to resize the images to 32x32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvgg16_xtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvgg16_xtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvgg16_xtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvgg16_xtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mpad\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/arraypad.py\u001b[0m in \u001b[0;36mpad\u001b[0;34m(array, pad_width, mode, **kwargs)\u001b[0m\n\u001b[1;32m    738\u001b[0m     \"\"\"\n\u001b[1;32m    739\u001b[0m     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m     \u001b[0mpad_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpad_width\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'i'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# First, we need to resize the images to 32x32\n",
        "vgg16_xtrain = [np.expand_dims(np.pad(np.resize(x, (28,28)), ((2,2),(2,2))), -1) for x in x_train]\n",
        "vgg16_xtrain = np.asarray(vgg16_xtrain)\n",
        "print(vgg16_xtrain.shape)\n",
        "\n",
        "vgg16_xtest = [np.expand_dims(np.pad(np.resize(x, (28,28)), ((2,2),(2,2))), -1) for x in x_test]\n",
        "vgg16_xtest = np.asarray(vgg16_xtest)\n",
        "print(vgg16_xtest.shape)\n",
        "\n",
        "def create_vgg16():\n",
        "  model = tf.keras.applications.VGG16(\n",
        "                                      include_top=True, #include the fully-connected classification layers\n",
        "                                      weights=None, # cannot use pre-trained weights for this custom task\n",
        "                                      input_shape=(32,32,1),\n",
        "                                      classes=10)\n",
        "  return model\n",
        "\n",
        "\n",
        "vgg16_mdl = create_vgg16()\n",
        "vgg16_mdl.compile(optimizer=\"adam\", #standard optimizer\n",
        "                    loss=\"categorical_crossentropy\", #standard for multi-class problems\n",
        "                    metrics=[\"accuracy\"])\n",
        "vgg16_mdl.summary()\n",
        "\n",
        "vgg16_mdl_history = vgg16_mdl.fit(x=vgg16_xtrain, \n",
        "                                    y=y_train_categorical, \n",
        "                                    epochs=1,\n",
        "                                    validation_split=0.1)\n",
        "\n",
        "# Evaluate the model.\n",
        "# We'll just evaluate accuracy for now. Later we'll abstract this to its own function\n",
        "# and evaluate accuracy, f1, and area under the ROC curve \n",
        "predictions_vgg16_mdl = vgg16_mdl.predict(vgg16_xtrain)\n",
        "print(\"VGG 16: \", calculate_metrics(y_true=y_test, y_pred=predictions_vgg16_mdl) )\n",
        "visualize_training(vgg16_mdl_history.history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_9DWz1X3xCp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Student Copy of CSC4220_DeepLearningLab",
      "provenance": []
    },
    "interpreter": {
      "hash": "1a29aef9348865955ee6bb3d80a3a53963af477277b943c49e3bad4179e2c33b"
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
